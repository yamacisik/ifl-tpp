{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:20:48.797775Z",
     "start_time": "2021-07-07T21:20:48.788763Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data import Sequence,SequenceDataset,get_inter_times,create_seq_data_set\n",
    "from model import LogNormMix,LogNormalMixtureDistribution\n",
    "from evaluation import get_prediction_for_all_events, get_prediction_for_last_events\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "\n",
    "from util import clamp_preserve_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:20:39.879293Z",
     "start_time": "2021-07-02T06:20:39.832546Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = 'data/simulated/hawkes_synthetic_random_2d_20191130-180837.pkl'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "dataset = np.load(dataset_name,allow_pickle = True)\n",
    "\n",
    "## Modify the dataset for IFTPL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:20:40.624945Z",
     "start_time": "2021-07-02T06:20:40.609729Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "# for i  in range(len(dataset['timestamps'])):\n",
    "for i  in range(400):\n",
    "    sequence = {'t_start':0,'t_end' :200, 'arrival_times':dataset['timestamps'][i].tolist(),'marks' :dataset['types'][i].tolist()}\n",
    "    \n",
    "    sequences.append(sequence)\n",
    "    \n",
    "simulated_data = {'sequences':sequences,'num_marks':2}\n",
    "\n",
    "# np.save('data/simulated/sample_hawkes',simulated_data,allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:39:16.963211Z",
     "start_time": "2021-07-02T06:39:16.902822Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# Model config\n",
    "context_size = 64                 # Size of the RNN hidden vector\n",
    "mark_embedding_size = 32          # Size of the mark embedding (used as RNN input)\n",
    "num_mix_components = 64           # Number of components for a mixture model\n",
    "rnn_type = \"GRU\"                  # What RNN to use as an encoder {\"RNN\", \"GRU\", \"LSTM\"}\n",
    "\n",
    "# Training config\n",
    "batch_size = 32        # Number of sequences in a batch\n",
    "regularization = 1e-5  # L2 regularization parameter\n",
    "learning_rate = 1e-3   # Learning rate for Adam optimizer\n",
    "max_epochs = 5      # For how many epochs to train\n",
    "display_step = 1      # Display training statistics after every display_step\n",
    "patience = 50          # After how many consecutive epochs without improvement of val loss to stop training\n",
    "\n",
    "test_dataset = {}\n",
    "test_dataset['sequences']= [{'t_start': 0,'t_end': 10,'arrival_times': [1,2],'marks':[0,1]},\n",
    "                            {'t_start': 0,'t_end': 10,'arrival_times': [1,3,5],'marks':[0,1,0]},\n",
    "                            {'t_start': 0,'t_end': 10,'arrival_times': [1,7,8,9],'marks':[0,1,1,0]},\n",
    "                            {'t_start': 0,   't_end': 10,'arrival_times': [5,9],'marks':[1,1]}  ]\n",
    "\n",
    "def get_inter_times(seq: dict):\n",
    "    \"\"\"Get inter-event times from a sequence.\"\"\"\n",
    "    return np.ediff1d(np.concatenate([[seq[\"t_start\"]], seq[\"arrival_times\"], [seq[\"t_end\"]]]))\n",
    "\n",
    "sequences = [\n",
    "    Sequence(\n",
    "        inter_times=get_inter_times(seq),\n",
    "        marks=seq.get(\"marks\"),\n",
    "        t_start=seq.get(\"t_start\"),\n",
    "        t_end=seq.get(\"t_end\")\n",
    "    )\n",
    "    for seq in simulated_data[\"sequences\"]\n",
    "]\n",
    "\n",
    "seed = 0\n",
    "batch_size = 5\n",
    "dataset = SequenceDataset(sequences=sequences, num_marks=2)\n",
    "\n",
    "d_train, d_val, d_test = dataset.train_val_test_split(seed=None,shuffle = False)\n",
    "\n",
    "training_events= d_train.total_num_events\n",
    "\n",
    "dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# dataset_name = 'data/stack_overflow.pkl'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "# dataset = torch.load(dataset_name)\n",
    "\n",
    "\n",
    "# def get_inter_times(seq: dict):\n",
    "#     \"\"\"Get inter-event times from a sequence.\"\"\"\n",
    "#     return np.ediff1d(np.concatenate([[seq[\"t_start\"]], seq[\"arrival_times\"], [seq[\"t_end\"]]]))\n",
    "\n",
    "\n",
    "# sequences = [\n",
    "#     Sequence(\n",
    "#         inter_times=get_inter_times(seq),\n",
    "#         marks=seq.get(\"marks\"),\n",
    "#         t_start=seq.get(\"t_start\"),\n",
    "#         t_end=seq.get(\"t_end\")\n",
    "#     )\n",
    "#     for seq in dataset[\"sequences\"]\n",
    "# ]\n",
    "# dataset = SequenceDataset(sequences=sequences, num_marks=dataset.get(\"num_marks\", 1))\n",
    "\n",
    "# d_train, d_val, d_test = dataset.train_val_test_split(seed=seed)\n",
    "\n",
    "# dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=True)\n",
    "# dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "# dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:39:23.795033Z",
     "start_time": "2021-07-02T06:39:23.766640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the model\n",
    "print('Building model...')\n",
    "mean_log_inter_time, std_log_inter_time = d_train.get_inter_time_statistics()\n",
    "\n",
    "model =LogNormMix(\n",
    "    num_marks=d_train.num_marks,\n",
    "    mean_log_inter_time=mean_log_inter_time,\n",
    "    std_log_inter_time=std_log_inter_time,\n",
    "    context_size=context_size,\n",
    "    mark_embedding_size=mark_embedding_size,\n",
    "    rnn_type=rnn_type,\n",
    "    num_mix_components=num_mix_components,\n",
    ")\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), weight_decay=regularization, lr=learning_rate)\n",
    "# Traning\n",
    "print('Starting training...')\n",
    "\n",
    "def aggregate_loss_over_dataloader(dl):\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            total_loss += -model.log_prob(batch).sum()\n",
    "            total_count += batch.mask.sum().item()\n",
    "    return total_loss / total_count\n",
    "\n",
    "\n",
    "impatient = 0\n",
    "best_loss = np.inf\n",
    "best_model = deepcopy(model.state_dict())\n",
    "training_val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:40:45.901675Z",
     "start_time": "2021-07-02T06:39:28.919304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: Training loss = 1.8333, loss_val = 1.8100\n",
      "Epoch    1: Training loss = 1.7969, loss_val = 1.8056\n",
      "Epoch    2: Training loss = 1.7926, loss_val = 1.8009\n",
      "Epoch    3: Training loss = 1.7895, loss_val = 1.7984\n",
      "Epoch    4: Training loss = 1.7871, loss_val = 1.7972\n",
      "Negative log-likelihood:\n",
      " - Train: 1.8\n",
      " - Val:   1.8\n",
      " - Test:  1.7\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    model.train()\n",
    "    for batch in dl_train:\n",
    "        opt.zero_grad()\n",
    "        # loss = -model.log_prob(batch)\n",
    "        loss = -model.log_prob(batch).sum()\n",
    "        loss.backward()\n",
    "        epoch_train_loss += loss.detach()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_val = aggregate_loss_over_dataloader(dl_val)\n",
    "        loss_test = aggregate_loss_over_dataloader(dl_test)\n",
    "\n",
    "        training_val_losses.append(loss_val)\n",
    "\n",
    "    if (best_loss - loss_val) < 1e-4:\n",
    "        impatient += 1\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    else:\n",
    "        best_loss = loss_val\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "        impatient = 0\n",
    "\n",
    "    if impatient >= patience:\n",
    "        print(f'Breaking due to early stopping at epoch {epoch}')\n",
    "        break\n",
    "    \n",
    "    \n",
    "    epoch_train_loss = epoch_train_loss/training_events\n",
    "\n",
    "    if epoch % display_step == 0:\n",
    "        print(f\"Epoch {epoch:4d}: Training loss = {epoch_train_loss.item():.4f}, loss_val = {loss_val:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# All training & testing sequences stacked into a single batch\n",
    "with torch.no_grad():\n",
    "    final_loss_train = aggregate_loss_over_dataloader(dl_train)\n",
    "    final_loss_val = aggregate_loss_over_dataloader(dl_val)\n",
    "    final_loss_test = aggregate_loss_over_dataloader(dl_test)\n",
    "\n",
    "print(f'Negative log-likelihood:\\n'\n",
    "      f' - Train: {final_loss_train:.4f}\\n'\n",
    "      f' - Val:   {final_loss_val:.4f}\\n'\n",
    "      f' - Test:  {final_loss_test:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:42.612274Z",
     "start_time": "2021-07-02T06:47:42.587254Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction_for_all_events(model,dl):\n",
    "    \n",
    "    total_num_events = dl.dataset.total_num_events\n",
    "    all_event_time_predictions = []\n",
    "    all_event_time_values = []\n",
    "    all_actual_marks = []\n",
    "    all_predicted_marks = []\n",
    "    \n",
    "    \n",
    "    for batch in dl:\n",
    "        \n",
    "        lengths = batch.mask.sum(-1)-1 ## Minus 1 because they also calculate the survival for the end of time.\n",
    "\n",
    "        \n",
    "        features = model.get_features(batch)\n",
    "        context = model.get_context(features)\n",
    "        inter_time_dist = model.get_inter_time_dist(context)\n",
    "        inter_times = batch.inter_times.clamp(1e-10)\n",
    "        \n",
    "        ## Arrival Time Prediction\n",
    "        predicted_times = inter_time_dist.mean        \n",
    "        all_predicted_times = torch.nn.utils.rnn.pack_padded_sequence(predicted_times.T,lengths,batch_first=False,enforce_sorted=False)[0]\n",
    "        all_actual_times = torch.nn.utils.rnn.pack_padded_sequence(inter_times.T,lengths,batch_first=False,enforce_sorted=False)[0]\n",
    "        all_event_time_values.append(all_actual_times[:-1])\n",
    "        all_event_time_predictions.append(all_predicted_times[:-1])\n",
    "        \n",
    "\n",
    "\n",
    "#         ## Mark Prediction\n",
    "        predicted_marks= torch.log_softmax(model.mark_linear(context), dim=-1).argmax(-1)\n",
    "        predicted_marks = torch.nn.utils.rnn.pack_padded_sequence(predicted_marks.T,lengths,batch_first=False,enforce_sorted=False)[0]\n",
    "        actual_marks = torch.nn.utils.rnn.pack_padded_sequence(batch.marks.T,lengths,batch_first=False,enforce_sorted =False)[0]\n",
    "        all_actual_marks.append(actual_marks)\n",
    "        all_predicted_marks.append(predicted_marks)\n",
    "        \n",
    "#         all_event_accuracy = (predicted_marks ==batch.marks)*batch.mask\n",
    "#         all_event_accuracies.append(all_event_accuracy)\n",
    "#         last_event_accuracy = all_event_accuracy[x_index,y_index]\n",
    "#         last_event_accuracies.append(last_event_accuracy)\n",
    "        \n",
    "    \n",
    "#     last_event_rmse = (torch.cat(last_event_errors,axis = 0)**2).mean().sqrt()\n",
    "#     all_event_rmse = ((torch.cat(total_errors,-1)**2).sum()/total_num_events).sqrt()\n",
    "#     all_event_accuracy = torch.cat(all_event_accuracies,-1).sum()/total_num_events\n",
    "#     last_event_accuracy = torch.cat(last_event_accuracies,0)\n",
    "# #     last_event_accuracy = last_event_accuracy.sum()/len(last_event_accuracy)\n",
    "    \n",
    "#     return last_event_rmse,all_event_rmse,all_event_accuracy,last_event_accuracy\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_event_time_values = torch.cat(all_event_time_values)\n",
    "    all_event_time_predictions = torch.cat(all_event_time_predictions)\n",
    "    all_actual_marks = torch.cat(all_actual_marks)\n",
    "    all_predicted_marks = torch.cat(all_predicted_marks)\n",
    "    \n",
    "    return all_event_time_values,all_event_time_predictions,all_actual_marks,all_predicted_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:55.177158Z",
     "start_time": "2021-07-02T06:47:55.157159Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction_for_last_events(model,dl):\n",
    "    \n",
    "    event_time_predictions = []\n",
    "    event_time_values = []\n",
    "    actual_marks = []\n",
    "    predicted_marks = []\n",
    "    \n",
    "    \n",
    "    for batch in dl:\n",
    "        \n",
    "        y_index = batch.mask.sum(-1).long() -1  ## Minus 1 because they also calculate the survival for the end of time.\n",
    "        features = model.get_features(batch)\n",
    "        context = model.get_context(features)\n",
    "        inter_time_dist = model.get_inter_time_dist(context)\n",
    "        inter_times = batch.inter_times.clamp(1e-10)\n",
    "        x_index = torch.arange(0,len(inter_times))\n",
    "        \n",
    "        ## Arrival Time Prediction\n",
    "        actual_time = inter_times[x_index,y_index]   \n",
    "        predicted_time = inter_time_dist.mean[x_index,y_index]     \n",
    "\n",
    "        ## Mark Prediction\n",
    "        actual_mark = batch.marks[x_index,y_index]\n",
    "        predicted_mark= torch.log_softmax(model.mark_linear(context), dim=-1).argmax(-1)[x_index,y_index]\n",
    "        \n",
    "        event_time_predictions.append(predicted_time)\n",
    "        event_time_values.append(actual_time)\n",
    "        actual_marks.append(actual_mark)\n",
    "        predicted_marks.append(predicted_mark)\n",
    "    \n",
    "    event_time_values = torch.cat(event_time_values)\n",
    "    event_time_predictions = torch.cat(event_time_predictions)\n",
    "    actual_marks = torch.cat(actual_marks)\n",
    "    predicted_marks = torch.cat(predicted_marks)\n",
    "    \n",
    "    return event_time_values,event_time_predictions,actual_marks,predicted_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:36:44.653532Z",
     "start_time": "2021-07-02T07:36:44.635534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.marks.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:56.679048Z",
     "start_time": "2021-07-02T06:47:56.294883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218.84422302246094\n",
      "0.550259965337955\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_all_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy())\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:57.979251Z",
     "start_time": "2021-07-02T06:47:57.597646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.017912864685059\n",
      "0.4571428571428572\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_last_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy())\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:52:42.635352Z",
     "start_time": "2021-07-02T07:52:42.617299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.marks.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:33:17.097874Z",
     "start_time": "2021-07-02T20:33:16.378156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_name = 'simulated'\n",
    "num_marks = 2\n",
    "dataset_path = 'data/' + dataset_name + '/'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "\n",
    "with open(dataset_path + 'train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(dataset_path + 'valid.pkl', 'rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(dataset_path + 'test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "\n",
    "def create_seq_data_set(dataset, num_marks,device):\n",
    "    sequences = [\n",
    "        Sequence(\n",
    "            inter_times=get_inter_times(seq),\n",
    "            marks=seq.get(\"marks\"),\n",
    "            t_start=seq.get(\"t_start\"),\n",
    "            t_end=seq.get(\"t_end\"),device = device\n",
    "        )\n",
    "        for seq in dataset[\"sequences\"]\n",
    "    ]\n",
    "    dataset = SequenceDataset(sequences=sequences, num_marks=num_marks)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "device ='cpu'\n",
    "d_train = create_seq_data_set(train, num_marks,device)\n",
    "d_val = create_seq_data_set(valid, num_marks,device)\n",
    "d_test = create_seq_data_set(test, num_marks,device)\n",
    "\n",
    "dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "print('Building model...')\n",
    "mean_log_inter_time, std_log_inter_time = d_train.get_inter_time_statistics()\n",
    "\n",
    "model =LogNormMix(\n",
    "    num_marks=d_train.num_marks,\n",
    "    mean_log_inter_time=mean_log_inter_time,\n",
    "    std_log_inter_time=std_log_inter_time,\n",
    "    context_size=context_size,\n",
    "    mark_embedding_size=mark_embedding_size,\n",
    "    rnn_type=rnn_type,\n",
    "    num_mix_components=num_mix_components,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model_dict =torch.load('intensity_free_modelsimulated',map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:33:52.771400Z",
     "start_time": "2021-07-02T20:33:49.743532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.669763565063477\n",
      "0.588495575221239\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_last_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy())\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mimic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T20:42:20.465617Z",
     "start_time": "2021-07-07T20:42:20.415399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 65\n",
    "dataset_name = 'mimic'\n",
    "num_marks = 75\n",
    "dataset_path = 'data/' + dataset_name + '/'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "\n",
    "with open(dataset_path + 'train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(dataset_path + 'valid.pkl', 'rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(dataset_path + 'test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device ='cpu'\n",
    "d_train = create_seq_data_set(train, num_marks,device)\n",
    "d_val = create_seq_data_set(valid, num_marks,device)\n",
    "d_test = create_seq_data_set(test, num_marks,device)\n",
    "\n",
    "dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "context_size = 64  # Size of the RNN hidden vector\n",
    "mark_embedding_size = 32  # Size of the mark embedding (used as RNN input)\n",
    "num_mix_components = 64  # Number of components for a mixture model\n",
    "rnn_type = \"GRU\"  # What RNN to use as an encoder {\"RNN\", \"GRU\", \"LSTM\"}\n",
    "\n",
    "# Training config\n",
    "batch_size = 32  # Number of sequences in a batch\n",
    "regularization = 1e-5  # L2 regularization parameter\n",
    "learning_rate = 1e-4  # Learning rate for Adam optimizer\n",
    "max_epochs = 5  # For how many epochs to train\n",
    "display_step = 5  # Display training statistics after every display_step\n",
    "patience = 50  # After how many consecutive epochs without improvement of val loss to stop training\n",
    "\n",
    "# Define the model\n",
    "print('Building model...')\n",
    "mean_log_inter_time, std_log_inter_time = d_train.get_inter_time_statistics()\n",
    "\n",
    "model =LogNormMix(\n",
    "    num_marks=d_train.num_marks,\n",
    "    mean_log_inter_time=mean_log_inter_time,\n",
    "    std_log_inter_time=std_log_inter_time,\n",
    "    context_size=context_size,\n",
    "    mark_embedding_size=mark_embedding_size,\n",
    "    rnn_type=rnn_type,\n",
    "    num_mix_components=num_mix_components,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model_dict =torch.load('intensity_free_modelmimic',map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T20:42:21.766537Z",
     "start_time": "2021-07-07T20:42:21.736657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "0.8769230769230769\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_last_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy(),average = 'micro')\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T20:56:30.950896Z",
     "start_time": "2021-07-07T20:56:30.933955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1749e+02, 4.3881e-01, 3.1367e+01, 1.4356e+07, 6.0000e+00, 1.6093e+00,\n",
       "        7.5512e+00, 9.4867e+00, 6.8709e+00, 6.4088e+02, 5.1162e+07, 1.5415e+00,\n",
       "        4.3675e-01, 6.8879e-01, 2.8085e+00, 5.0541e-01,        inf, 1.0922e+00,\n",
       "        5.8352e+00, 1.8536e+00, 7.4858e+00, 6.9754e+00, 4.5153e+03, 6.8216e+00,\n",
       "        1.4159e+00,        inf, 3.4644e+00, 5.3584e-01,        inf, 5.9234e-01,\n",
       "        1.7005e+00, 4.4791e-01, 2.8797e+00, 1.6537e+00, 1.5182e+00, 1.5238e+00,\n",
       "        5.4567e-01, 7.1876e+00, 2.2442e+00, 3.3967e+00, 3.3967e+00, 1.6982e+00,\n",
       "        4.9693e-01, 2.9801e+00, 3.8397e+00, 7.3661e+00, 1.2535e+00, 4.8983e-01,\n",
       "        5.7855e-01, 1.0930e+00, 2.0347e+09, 9.0467e+00, 3.6998e-01, 3.8227e+00,\n",
       "        7.1085e+00, 5.9449e+00, 3.0415e+00, 4.5754e-01, 7.6509e+00, 1.6982e+00,\n",
       "        5.3584e-01, 7.8676e+00, 1.6659e+00, 5.2245e-01,        inf],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T20:43:25.961186Z",
     "start_time": "2021-07-07T20:43:25.938260Z"
    }
   },
   "outputs": [],
   "source": [
    "features = model.get_features(batch)\n",
    "context = model.get_context(features)\n",
    "inter_time_dist = model.get_inter_time_dist(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T20:57:06.897603Z",
     "start_time": "2021-07-07T20:57:06.888639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-10, 3.0769e-01, 3.4615e-01, 5.3462e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.inter_times[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:00:06.189329Z",
     "start_time": "2021-07-07T21:00:06.174404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-10, 3.0769e-01, 3.4615e-01, 5.3462e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.inter_times[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T20:59:43.471161Z",
     "start_time": "2021-07-07T20:59:43.462185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 20, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.marks[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T20:59:15.395085Z",
     "start_time": "2021-07-07T20:59:15.388107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.mask[-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T20:57:28.709511Z",
     "start_time": "2021-07-07T20:57:28.692564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3683e+30, 2.6320e+35,        inf,        inf, 7.9683e+01, 2.1732e+01,\n",
       "        1.0686e+01, 9.4735e+00, 9.6479e+00, 1.0049e+01, 1.0429e+01, 1.0733e+01],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_time_dist.mean[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T20:44:30.058227Z",
     "start_time": "2021-07-07T20:44:30.043277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-10, 3.0769e-01, 3.4615e-01, 5.3462e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.inter_times[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T20:42:26.607972Z",
     "start_time": "2021-07-07T20:42:26.593001Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch in dl_test:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:22:56.703758Z",
     "start_time": "2021-07-07T21:22:56.681851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(350.4573, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_index = batch.mask.sum(\n",
    "    -1).long() - 1  ## Minus 1 because they also calculate the survival for the end of time.\n",
    "features = model.get_features(batch)\n",
    "context = model.get_context(features)\n",
    "inter_time_dist = model.get_inter_time_dist(context)\n",
    "\n",
    "\n",
    "raw_params = model.linear(context)  # (batch_size, seq_len, 3 * num_mix_components)\n",
    "# Slice the tensor to get the parameters of the mixture\n",
    "locs = raw_params[..., :model.num_mix_components]\n",
    "log_scales = raw_params[..., model.num_mix_components: (2 * model.num_mix_components)]\n",
    "log_weights = raw_params[..., (2 * model.num_mix_components):]\n",
    "\n",
    "log_scales = clamp_preserve_gradients(log_scales, -5.0, 1.0)\n",
    "log_weights = torch.log_softmax(log_weights, dim=-1)\n",
    "inter_time_dist= LogNormalMixtureDistribution(\n",
    "    locs=locs,\n",
    "    log_scales=log_scales,\n",
    "    log_weights=log_weights,\n",
    "    mean_log_inter_time=model.mean_log_inter_time,\n",
    "    std_log_inter_time=model.std_log_inter_time\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "a = inter_time_dist.std_log_inter_time\n",
    "b = inter_time_dist.mean_log_inter_time\n",
    "loc = inter_time_dist.base_dist._component_distribution.loc\n",
    "variance = inter_time_dist.base_dist._component_distribution.variance\n",
    "log_weights = inter_time_dist.base_dist._mixture_distribution.logits\n",
    "(log_weights + a * loc + b + 0.5 * a ** 2 * variance).logsumexp(-1)[-1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:23:02.165852Z",
     "start_time": "2021-07-07T21:23:02.159894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.7428)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.std_log_inter_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\sum_k w_k \\exp(a * \\mu_k + b + a^2 * s_k^2 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:16:18.200766Z",
     "start_time": "2021-07-07T21:16:18.184785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.3827e-02,  2.4959e+00,  5.6551e+00,  9.6110e-01,  1.6284e+01,\n",
       "         1.0101e+00,  3.3274e+00,  4.1098e+01,  7.5747e+00,  1.8718e+00,\n",
       "        -1.3737e+00,  3.4822e+01,  8.1448e+00,  1.8703e+01, -2.8655e+00,\n",
       "         5.7551e+02,  6.7150e-01, -1.7918e+00,  4.2643e+00,  6.0229e+00,\n",
       "         1.8080e+01,  2.7982e+00,  7.7480e+00,  2.0706e+00,  9.0482e-01,\n",
       "         1.3496e+00,  1.0050e+01,  1.2869e+00,  3.7434e+00,  5.2432e+00,\n",
       "        -7.3683e-01,  7.0787e-01,  9.2646e+00,  2.1809e-01,  2.8906e+00,\n",
       "         1.1255e+00, -1.1422e+00, -5.0909e-01,  3.0159e+00,  5.1239e+00,\n",
       "         6.7002e-01,  1.7750e+02,  2.3232e+00,  2.4128e+00,  1.8722e+00,\n",
       "         5.1205e+00,  7.2887e+00,  2.4980e-01,  1.3400e+00, -3.3546e+00,\n",
       "         3.8095e-01,  1.0204e+01, -3.1551e+00,  2.8819e-01,  6.4451e+01,\n",
       "        -2.5328e+00,  1.1071e+01, -7.6883e-01,  3.5442e+01, -4.6588e-01,\n",
       "        -2.8729e+00,  1.5414e+00,  3.0847e+02, -2.2779e+00],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a * loc[-1][2]) + b+ (0.5*variance[-1][2] * a**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:06:57.172361Z",
     "start_time": "2021-07-07T21:06:57.152414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.7719e+00, -2.4933e+00,  3.2127e-01, -2.6305e+00,  1.0638e+01,\n",
       "        -3.2886e+00, -1.0176e+00,  3.4976e+01,  2.0127e+00, -2.0520e+00,\n",
       "        -5.5669e+00,  2.8941e+01,  2.8677e+00,  1.3045e+01, -6.8826e+00,\n",
       "         5.6830e+02, -3.5138e+00, -5.4087e+00, -9.0726e-01,  1.1996e+00,\n",
       "         1.2257e+01, -1.5404e+00,  2.2187e+00, -2.3010e+00, -2.3071e+00,\n",
       "        -1.6139e+00,  4.6739e+00, -1.7721e+00, -1.1501e+00,  3.2883e-01,\n",
       "        -4.4762e+00, -3.1934e+00,  3.7948e+00, -3.6728e+00, -1.2871e+00,\n",
       "        -3.8649e+00, -4.4348e+00, -4.7654e+00, -1.4894e+00, -5.6178e-01,\n",
       "        -3.3523e+00,  1.7069e+02, -2.0465e+00, -2.2022e+00, -2.6822e+00,\n",
       "         4.8712e-02,  1.9735e+00, -3.6402e+00, -2.7830e+00, -6.9390e+00,\n",
       "        -4.2677e+00,  4.9924e+00, -6.5031e+00, -3.9428e+00,  5.8077e+01,\n",
       "        -6.1407e+00,  5.1859e+00, -4.4552e+00,  2.9361e+01, -3.4964e+00,\n",
       "        -6.2605e+00, -2.7168e+00,  3.0133e+02, -5.2975e+00],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(log_weights + a * loc + b + 0.5 * a ** 2 * variance)[-1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:06:22.590410Z",
     "start_time": "2021-07-07T21:06:22.578481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(568.3008, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = inter_time_dist.std_log_inter_time\n",
    "b = inter_time_dist.mean_log_inter_time\n",
    "loc = inter_time_dist.base_dist._component_distribution.loc\n",
    "variance = inter_time_dist.base_dist._component_distribution.variance\n",
    "log_weights = inter_time_dist.base_dist._mixture_distribution.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
