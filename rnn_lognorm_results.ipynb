{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T22:02:37.706571Z",
     "start_time": "2021-07-07T22:02:36.592037Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data import Sequence,SequenceDataset,get_inter_times,create_seq_data_set\n",
    "from model import LogNormMix,LogNormalMixtureDistribution\n",
    "from evaluation import get_prediction_for_all_events, get_prediction_for_last_events\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "\n",
    "from util import clamp_preserve_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:20:39.879293Z",
     "start_time": "2021-07-02T06:20:39.832546Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = 'data/simulated/hawkes_synthetic_random_2d_20191130-180837.pkl'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "dataset = np.load(dataset_name,allow_pickle = True)\n",
    "\n",
    "## Modify the dataset for IFTPL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:20:40.624945Z",
     "start_time": "2021-07-02T06:20:40.609729Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "# for i  in range(len(dataset['timestamps'])):\n",
    "for i  in range(400):\n",
    "    sequence = {'t_start':0,'t_end' :200, 'arrival_times':dataset['timestamps'][i].tolist(),'marks' :dataset['types'][i].tolist()}\n",
    "    \n",
    "    sequences.append(sequence)\n",
    "    \n",
    "simulated_data = {'sequences':sequences,'num_marks':2}\n",
    "\n",
    "# np.save('data/simulated/sample_hawkes',simulated_data,allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:39:16.963211Z",
     "start_time": "2021-07-02T06:39:16.902822Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# Model config\n",
    "context_size = 64                 # Size of the RNN hidden vector\n",
    "mark_embedding_size = 32          # Size of the mark embedding (used as RNN input)\n",
    "num_mix_components = 64           # Number of components for a mixture model\n",
    "rnn_type = \"GRU\"                  # What RNN to use as an encoder {\"RNN\", \"GRU\", \"LSTM\"}\n",
    "\n",
    "# Training config\n",
    "batch_size = 32        # Number of sequences in a batch\n",
    "regularization = 1e-5  # L2 regularization parameter\n",
    "learning_rate = 1e-3   # Learning rate for Adam optimizer\n",
    "max_epochs = 5      # For how many epochs to train\n",
    "display_step = 1      # Display training statistics after every display_step\n",
    "patience = 50          # After how many consecutive epochs without improvement of val loss to stop training\n",
    "\n",
    "test_dataset = {}\n",
    "test_dataset['sequences']= [{'t_start': 0,'t_end': 10,'arrival_times': [1,2],'marks':[0,1]},\n",
    "                            {'t_start': 0,'t_end': 10,'arrival_times': [1,3,5],'marks':[0,1,0]},\n",
    "                            {'t_start': 0,'t_end': 10,'arrival_times': [1,7,8,9],'marks':[0,1,1,0]},\n",
    "                            {'t_start': 0,   't_end': 10,'arrival_times': [5,9],'marks':[1,1]}  ]\n",
    "\n",
    "def get_inter_times(seq: dict):\n",
    "    \"\"\"Get inter-event times from a sequence.\"\"\"\n",
    "    return np.ediff1d(np.concatenate([[seq[\"t_start\"]], seq[\"arrival_times\"], [seq[\"t_end\"]]]))\n",
    "\n",
    "sequences = [\n",
    "    Sequence(\n",
    "        inter_times=get_inter_times(seq),\n",
    "        marks=seq.get(\"marks\"),\n",
    "        t_start=seq.get(\"t_start\"),\n",
    "        t_end=seq.get(\"t_end\")\n",
    "    )\n",
    "    for seq in simulated_data[\"sequences\"]\n",
    "]\n",
    "\n",
    "seed = 0\n",
    "batch_size = 5\n",
    "dataset = SequenceDataset(sequences=sequences, num_marks=2)\n",
    "\n",
    "d_train, d_val, d_test = dataset.train_val_test_split(seed=None,shuffle = False)\n",
    "\n",
    "training_events= d_train.total_num_events\n",
    "\n",
    "dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# dataset_name = 'data/stack_overflow.pkl'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "# dataset = torch.load(dataset_name)\n",
    "\n",
    "\n",
    "# def get_inter_times(seq: dict):\n",
    "#     \"\"\"Get inter-event times from a sequence.\"\"\"\n",
    "#     return np.ediff1d(np.concatenate([[seq[\"t_start\"]], seq[\"arrival_times\"], [seq[\"t_end\"]]]))\n",
    "\n",
    "\n",
    "# sequences = [\n",
    "#     Sequence(\n",
    "#         inter_times=get_inter_times(seq),\n",
    "#         marks=seq.get(\"marks\"),\n",
    "#         t_start=seq.get(\"t_start\"),\n",
    "#         t_end=seq.get(\"t_end\")\n",
    "#     )\n",
    "#     for seq in dataset[\"sequences\"]\n",
    "# ]\n",
    "# dataset = SequenceDataset(sequences=sequences, num_marks=dataset.get(\"num_marks\", 1))\n",
    "\n",
    "# d_train, d_val, d_test = dataset.train_val_test_split(seed=seed)\n",
    "\n",
    "# dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=True)\n",
    "# dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "# dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:39:23.795033Z",
     "start_time": "2021-07-02T06:39:23.766640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the model\n",
    "print('Building model...')\n",
    "mean_log_inter_time, std_log_inter_time = d_train.get_inter_time_statistics()\n",
    "\n",
    "model =LogNormMix(\n",
    "    num_marks=d_train.num_marks,\n",
    "    mean_log_inter_time=mean_log_inter_time,\n",
    "    std_log_inter_time=std_log_inter_time,\n",
    "    context_size=context_size,\n",
    "    mark_embedding_size=mark_embedding_size,\n",
    "    rnn_type=rnn_type,\n",
    "    num_mix_components=num_mix_components,\n",
    ")\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), weight_decay=regularization, lr=learning_rate)\n",
    "# Traning\n",
    "print('Starting training...')\n",
    "\n",
    "def aggregate_loss_over_dataloader(dl):\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            total_loss += -model.log_prob(batch).sum()\n",
    "            total_count += batch.mask.sum().item()\n",
    "    return total_loss / total_count\n",
    "\n",
    "\n",
    "impatient = 0\n",
    "best_loss = np.inf\n",
    "best_model = deepcopy(model.state_dict())\n",
    "training_val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:40:45.901675Z",
     "start_time": "2021-07-02T06:39:28.919304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: Training loss = 1.8333, loss_val = 1.8100\n",
      "Epoch    1: Training loss = 1.7969, loss_val = 1.8056\n",
      "Epoch    2: Training loss = 1.7926, loss_val = 1.8009\n",
      "Epoch    3: Training loss = 1.7895, loss_val = 1.7984\n",
      "Epoch    4: Training loss = 1.7871, loss_val = 1.7972\n",
      "Negative log-likelihood:\n",
      " - Train: 1.8\n",
      " - Val:   1.8\n",
      " - Test:  1.7\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    model.train()\n",
    "    for batch in dl_train:\n",
    "        opt.zero_grad()\n",
    "        # loss = -model.log_prob(batch)\n",
    "        loss = -model.log_prob(batch).sum()\n",
    "        loss.backward()\n",
    "        epoch_train_loss += loss.detach()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_val = aggregate_loss_over_dataloader(dl_val)\n",
    "        loss_test = aggregate_loss_over_dataloader(dl_test)\n",
    "\n",
    "        training_val_losses.append(loss_val)\n",
    "\n",
    "    if (best_loss - loss_val) < 1e-4:\n",
    "        impatient += 1\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    else:\n",
    "        best_loss = loss_val\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "        impatient = 0\n",
    "\n",
    "    if impatient >= patience:\n",
    "        print(f'Breaking due to early stopping at epoch {epoch}')\n",
    "        break\n",
    "    \n",
    "    \n",
    "    epoch_train_loss = epoch_train_loss/training_events\n",
    "\n",
    "    if epoch % display_step == 0:\n",
    "        print(f\"Epoch {epoch:4d}: Training loss = {epoch_train_loss.item():.4f}, loss_val = {loss_val:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# All training & testing sequences stacked into a single batch\n",
    "with torch.no_grad():\n",
    "    final_loss_train = aggregate_loss_over_dataloader(dl_train)\n",
    "    final_loss_val = aggregate_loss_over_dataloader(dl_val)\n",
    "    final_loss_test = aggregate_loss_over_dataloader(dl_test)\n",
    "\n",
    "print(f'Negative log-likelihood:\\n'\n",
    "      f' - Train: {final_loss_train:.4f}\\n'\n",
    "      f' - Val:   {final_loss_val:.4f}\\n'\n",
    "      f' - Test:  {final_loss_test:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:42.612274Z",
     "start_time": "2021-07-02T06:47:42.587254Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction_for_all_events(model,dl):\n",
    "    \n",
    "    total_num_events = dl.dataset.total_num_events\n",
    "    all_event_time_predictions = []\n",
    "    all_event_time_values = []\n",
    "    all_actual_marks = []\n",
    "    all_predicted_marks = []\n",
    "    \n",
    "    \n",
    "    for batch in dl:\n",
    "        \n",
    "        lengths = batch.mask.sum(-1)-1 ## Minus 1 because they also calculate the survival for the end of time.\n",
    "\n",
    "        \n",
    "        features = model.get_features(batch)\n",
    "        context = model.get_context(features)\n",
    "        inter_time_dist = model.get_inter_time_dist(context)\n",
    "        inter_times = batch.inter_times.clamp(1e-10)\n",
    "        \n",
    "        ## Arrival Time Prediction\n",
    "        predicted_times = inter_time_dist.mean        \n",
    "        all_predicted_times = torch.nn.utils.rnn.pack_padded_sequence(predicted_times.T,lengths,batch_first=False,enforce_sorted=False)[0]\n",
    "        all_actual_times = torch.nn.utils.rnn.pack_padded_sequence(inter_times.T,lengths,batch_first=False,enforce_sorted=False)[0]\n",
    "        all_event_time_values.append(all_actual_times[:-1])\n",
    "        all_event_time_predictions.append(all_predicted_times[:-1])\n",
    "        \n",
    "\n",
    "\n",
    "#         ## Mark Prediction\n",
    "        predicted_marks= torch.log_softmax(model.mark_linear(context), dim=-1).argmax(-1)\n",
    "        predicted_marks = torch.nn.utils.rnn.pack_padded_sequence(predicted_marks.T,lengths,batch_first=False,enforce_sorted=False)[0]\n",
    "        actual_marks = torch.nn.utils.rnn.pack_padded_sequence(batch.marks.T,lengths,batch_first=False,enforce_sorted =False)[0]\n",
    "        all_actual_marks.append(actual_marks)\n",
    "        all_predicted_marks.append(predicted_marks)\n",
    "        \n",
    "#         all_event_accuracy = (predicted_marks ==batch.marks)*batch.mask\n",
    "#         all_event_accuracies.append(all_event_accuracy)\n",
    "#         last_event_accuracy = all_event_accuracy[x_index,y_index]\n",
    "#         last_event_accuracies.append(last_event_accuracy)\n",
    "        \n",
    "    \n",
    "#     last_event_rmse = (torch.cat(last_event_errors,axis = 0)**2).mean().sqrt()\n",
    "#     all_event_rmse = ((torch.cat(total_errors,-1)**2).sum()/total_num_events).sqrt()\n",
    "#     all_event_accuracy = torch.cat(all_event_accuracies,-1).sum()/total_num_events\n",
    "#     last_event_accuracy = torch.cat(last_event_accuracies,0)\n",
    "# #     last_event_accuracy = last_event_accuracy.sum()/len(last_event_accuracy)\n",
    "    \n",
    "#     return last_event_rmse,all_event_rmse,all_event_accuracy,last_event_accuracy\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_event_time_values = torch.cat(all_event_time_values)\n",
    "    all_event_time_predictions = torch.cat(all_event_time_predictions)\n",
    "    all_actual_marks = torch.cat(all_actual_marks)\n",
    "    all_predicted_marks = torch.cat(all_predicted_marks)\n",
    "    \n",
    "    return all_event_time_values,all_event_time_predictions,all_actual_marks,all_predicted_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:55.177158Z",
     "start_time": "2021-07-02T06:47:55.157159Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction_for_last_events(model,dl):\n",
    "    \n",
    "    event_time_predictions = []\n",
    "    event_time_values = []\n",
    "    actual_marks = []\n",
    "    predicted_marks = []\n",
    "    \n",
    "    \n",
    "    for batch in dl:\n",
    "        \n",
    "        y_index = batch.mask.sum(-1).long() -1  ## Minus 1 because they also calculate the survival for the end of time.\n",
    "        features = model.get_features(batch)\n",
    "        context = model.get_context(features)\n",
    "        inter_time_dist = model.get_inter_time_dist(context)\n",
    "        inter_times = batch.inter_times.clamp(1e-10)\n",
    "        x_index = torch.arange(0,len(inter_times))\n",
    "        \n",
    "        ## Arrival Time Prediction\n",
    "        actual_time = inter_times[x_index,y_index]   \n",
    "        predicted_time = inter_time_dist.mean[x_index,y_index]     \n",
    "\n",
    "        ## Mark Prediction\n",
    "        actual_mark = batch.marks[x_index,y_index]\n",
    "        predicted_mark= torch.log_softmax(model.mark_linear(context), dim=-1).argmax(-1)[x_index,y_index]\n",
    "        \n",
    "        event_time_predictions.append(predicted_time)\n",
    "        event_time_values.append(actual_time)\n",
    "        actual_marks.append(actual_mark)\n",
    "        predicted_marks.append(predicted_mark)\n",
    "    \n",
    "    event_time_values = torch.cat(event_time_values)\n",
    "    event_time_predictions = torch.cat(event_time_predictions)\n",
    "    actual_marks = torch.cat(actual_marks)\n",
    "    predicted_marks = torch.cat(predicted_marks)\n",
    "    \n",
    "    return event_time_values,event_time_predictions,actual_marks,predicted_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:36:44.653532Z",
     "start_time": "2021-07-02T07:36:44.635534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.marks.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:56.679048Z",
     "start_time": "2021-07-02T06:47:56.294883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218.84422302246094\n",
      "0.550259965337955\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_all_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy())\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:57.979251Z",
     "start_time": "2021-07-02T06:47:57.597646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.017912864685059\n",
      "0.4571428571428572\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_last_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy())\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:52:42.635352Z",
     "start_time": "2021-07-02T07:52:42.617299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.marks.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:33:17.097874Z",
     "start_time": "2021-07-02T20:33:16.378156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_name = 'simulated'\n",
    "num_marks = 2\n",
    "dataset_path = 'data/' + dataset_name + '/'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "\n",
    "with open(dataset_path + 'train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(dataset_path + 'valid.pkl', 'rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(dataset_path + 'test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "\n",
    "def create_seq_data_set(dataset, num_marks,device):\n",
    "    sequences = [\n",
    "        Sequence(\n",
    "            inter_times=get_inter_times(seq),\n",
    "            marks=seq.get(\"marks\"),\n",
    "            t_start=seq.get(\"t_start\"),\n",
    "            t_end=seq.get(\"t_end\"),device = device\n",
    "        )\n",
    "        for seq in dataset[\"sequences\"]\n",
    "    ]\n",
    "    dataset = SequenceDataset(sequences=sequences, num_marks=num_marks)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "device ='cpu'\n",
    "d_train = create_seq_data_set(train, num_marks,device)\n",
    "d_val = create_seq_data_set(valid, num_marks,device)\n",
    "d_test = create_seq_data_set(test, num_marks,device)\n",
    "\n",
    "dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "print('Building model...')\n",
    "mean_log_inter_time, std_log_inter_time = d_train.get_inter_time_statistics()\n",
    "\n",
    "model =LogNormMix(\n",
    "    num_marks=d_train.num_marks,\n",
    "    mean_log_inter_time=mean_log_inter_time,\n",
    "    std_log_inter_time=std_log_inter_time,\n",
    "    context_size=context_size,\n",
    "    mark_embedding_size=mark_embedding_size,\n",
    "    rnn_type=rnn_type,\n",
    "    num_mix_components=num_mix_components,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model_dict =torch.load('intensity_free_modelsimulated',map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:33:52.771400Z",
     "start_time": "2021-07-02T20:33:49.743532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.669763565063477\n",
      "0.588495575221239\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_last_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy())\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mimic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T22:02:41.135907Z",
     "start_time": "2021-07-07T22:02:41.057818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 65\n",
    "dataset_name = 'mimic'\n",
    "num_marks = 75\n",
    "dataset_path = 'data/' + dataset_name + '/'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "\n",
    "with open(dataset_path + 'train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(dataset_path + 'valid.pkl', 'rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(dataset_path + 'test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device ='cpu'\n",
    "d_train = create_seq_data_set(train, num_marks,device)\n",
    "d_val = create_seq_data_set(valid, num_marks,device)\n",
    "d_test = create_seq_data_set(test, num_marks,device)\n",
    "\n",
    "dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "context_size = 16  # Size of the RNN hidden vector\n",
    "mark_embedding_size = 32  # Size of the mark embedding (used as RNN input)\n",
    "num_mix_components = 32  # Number of components for a mixture model\n",
    "rnn_type = \"GRU\"  # What RNN to use as an encoder {\"RNN\", \"GRU\", \"LSTM\"}\n",
    "\n",
    "# Training config\n",
    "batch_size = 32  # Number of sequences in a batch\n",
    "regularization = 1e-5  # L2 regularization parameter\n",
    "learning_rate = 1e-4  # Learning rate for Adam optimizer\n",
    "max_epochs = 5  # For how many epochs to train\n",
    "display_step = 5  # Display training statistics after every display_step\n",
    "patience = 50  # After how many consecutive epochs without improvement of val loss to stop training\n",
    "\n",
    "# Define the model\n",
    "print('Building model...')\n",
    "mean_log_inter_time, std_log_inter_time = d_train.get_inter_time_statistics()\n",
    "\n",
    "model =LogNormMix(\n",
    "    num_marks=d_train.num_marks,\n",
    "    mean_log_inter_time=mean_log_inter_time,\n",
    "    std_log_inter_time=std_log_inter_time,\n",
    "    context_size=context_size,\n",
    "    mark_embedding_size=mark_embedding_size,\n",
    "    rnn_type=rnn_type,\n",
    "    num_mix_components=num_mix_components,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model_dict =torch.load('intensity_free_modelmimic',map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T22:03:12.365164Z",
     "start_time": "2021-07-07T22:03:12.340268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "0.8769230769230769\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_last_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy(),average = 'micro')\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T22:05:02.014831Z",
     "start_time": "2021-07-07T22:05:02.008884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.9191e+18,  3.3715e+16,  1.5317e+20,  3.1559e+07,  1.5131e+02,\n",
       "         7.3572e+14,  1.0394e+01,  1.8050e+17,  3.4576e+00,  3.4490e+14,\n",
       "         2.4278e+09,  4.3347e+14,  4.7945e+15,  1.7335e+16,  1.2004e+21,\n",
       "         1.5343e+15,  4.8086e+14,  3.0996e+06,  1.9846e+00,  2.0067e+05,\n",
       "         7.2873e+00,  3.8943e+00,  3.1081e+19,  2.0108e+03,  8.5178e+15,\n",
       "         2.1957e+15,  8.7142e+17,  2.4147e+16,  4.3640e+19,  9.1649e+07,\n",
       "         1.2588e+04,  1.5045e+15,  4.0996e+20,  1.3840e+15,  1.3342e+05,\n",
       "         2.4252e+15,  4.2184e+15,  1.6490e+00,  9.7951e+17,  1.2537e+04,\n",
       "         1.0783e+05,  6.6626e+14,  1.3722e+15,  8.0688e+17,  3.9652e+16,\n",
       "         1.8607e+00,  2.2581e+19,  8.5728e+15,  9.2153e+07,  1.4733e+20,\n",
       "         9.3126e+18,  2.5097e+19,  9.0085e+09,  1.6920e+16,  4.0240e+00,\n",
       "         3.5342e+01,  8.9426e+00,  3.2999e+15, -5.9202e-01,  8.1616e+15,\n",
       "         6.4392e+15,  3.7867e+00,  1.0388e+04,  6.0665e+14,  5.5599e+14],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((predicted_times - actual_times)/actual_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T22:03:36.546426Z",
     "start_time": "2021-07-07T22:03:36.515393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.7874e+37, 1.1367e+33,        inf, 9.9599e+14, 2.2894e+04, 5.4129e+29,\n",
       "        1.0803e+02, 3.2581e+34, 1.1955e+01, 1.1895e+29, 5.8942e+18, 1.8789e+29,\n",
       "        2.2988e+31, 3.0050e+32,        inf, 2.3540e+30, 2.3123e+29, 9.6073e+12,\n",
       "        3.9388e+00, 4.0267e+10, 5.3105e+01, 1.5165e+01,        inf, 4.0433e+06,\n",
       "        7.2552e+31, 4.8210e+30, 7.5937e+35, 5.8307e+32,        inf, 8.3996e+15,\n",
       "        1.5846e+08, 2.2634e+30,        inf, 1.9154e+30, 1.7800e+10, 5.8814e+30,\n",
       "        1.7795e+31, 2.7193e+00, 9.5944e+35, 1.5719e+08, 1.1627e+10, 4.4390e+29,\n",
       "        1.8830e+30, 6.5105e+35, 1.5723e+33, 3.4622e+00,        inf, 7.3492e+31,\n",
       "        8.4922e+15,        inf, 8.6725e+37,        inf, 8.1152e+19, 2.8629e+32,\n",
       "        1.6192e+01, 1.2490e+03, 7.9971e+01, 1.0889e+31, 3.5049e-01, 6.6612e+31,\n",
       "        4.1463e+31, 1.4339e+01, 1.0790e+08, 3.6802e+29, 3.0913e+29],\n",
       "       grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((predicted_times - actual_times)/actual_times)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:57:59.582077Z",
     "start_time": "2021-07-07T21:57:59.558175Z"
    }
   },
   "outputs": [],
   "source": [
    "features = model.get_features(batch)\n",
    "context = model.get_context(features)\n",
    "inter_time_dist = model.get_inter_time_dist(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T20:57:06.897603Z",
     "start_time": "2021-07-07T20:57:06.888639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-10, 3.0769e-01, 3.4615e-01, 5.3462e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.inter_times[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:00:06.189329Z",
     "start_time": "2021-07-07T21:00:06.174404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-10, 3.0769e-01, 3.4615e-01, 5.3462e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.inter_times[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:58:04.954948Z",
     "start_time": "2021-07-07T21:58:04.939952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 20, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.marks[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:58:05.424592Z",
     "start_time": "2021-07-07T21:58:05.413625Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-148-f30c47702c24>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-148-f30c47702c24>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    batch.mask[-1].\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "batch.mask[-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T22:02:08.188514Z",
     "start_time": "2021-07-07T22:02:08.169603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([       inf,        inf,        inf, 2.4276e+06, 5.8580e+00, 7.0210e+34,\n",
       "        1.0956e+00,        inf, 1.3716e+00, 6.1062e+15, 1.4007e+09, 7.4018e+33,\n",
       "               inf,        inf,        inf,        inf,        inf, 1.7882e+05,\n",
       "        7.2894e+00, 2.3154e+04, 1.1156e+00, 1.3177e+00,        inf, 3.4819e+02,\n",
       "        1.8182e+35,        inf,        inf,        inf,        inf, 1.4100e+07,\n",
       "        2.4452e+04,        inf,        inf, 6.0791e+34, 3.3354e+04, 7.7382e+33,\n",
       "               inf, 1.2226e+00,        inf, 2.0737e+04, 2.0737e+04, 4.9383e+33,\n",
       "               inf,        inf, 1.0253e+31, 1.1553e+00,        inf,        inf,\n",
       "        1.4177e+07,        inf,        inf,        inf, 3.4648e+08, 2.0014e+31,\n",
       "        1.2560e+00, 6.2899e+00, 1.0325e+01,        inf, 1.0670e+00, 4.9383e+33,\n",
       "               inf, 1.0126e+00, 2.4773e+04,        inf,        inf],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_index = batch.mask.sum(\n",
    "    -1).long() - 1  ## Minus 1 because they also calculate the survival for the end of time.\n",
    "features = model.get_features(batch)\n",
    "context = model.get_context(features)\n",
    "inter_time_dist = model.get_inter_time_dist(context)\n",
    "inter_times = batch.inter_times.clamp(1e-10)\n",
    "x_index = torch.arange(0, len(inter_times))\n",
    "\n",
    "## Arrival Time Prediction\n",
    "actual_time = inter_times[x_index, y_index]\n",
    "predicted_time = inter_time_dist.mean[x_index, y_index]\n",
    "predicted_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T22:02:08.657920Z",
     "start_time": "2021-07-07T22:02:08.639009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([      inf,       inf,       inf,       inf, 1127.3969,  157.7208,\n",
       "         202.5623,  392.1788,  820.6556, 1696.0892, 3375.1792, 6434.6396],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_time_dist.mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T22:00:50.724418Z",
     "start_time": "2021-07-07T22:00:50.705467Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch in dl_test:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T22:01:20.888848Z",
     "start_time": "2021-07-07T22:01:20.866906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.3453, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_index = batch.mask.sum(\n",
    "    -1).long() - 1  ## Minus 1 because they also calculate the survival for the end of time.\n",
    "features = model.get_features(batch)\n",
    "context = model.get_context(features)\n",
    "inter_time_dist = model.get_inter_time_dist(context)\n",
    "\n",
    "\n",
    "raw_params = model.linear(context)  # (batch_size, seq_len, 3 * num_mix_components)\n",
    "# Slice the tensor to get the parameters of the mixture\n",
    "locs = raw_params[..., :model.num_mix_components]\n",
    "log_scales = raw_params[..., model.num_mix_components: (2 * model.num_mix_components)]\n",
    "log_weights = raw_params[..., (2 * model.num_mix_components):]\n",
    "\n",
    "log_scales = clamp_preserve_gradients(log_scales, -5.0,0.0)\n",
    "log_weights = torch.log_softmax(log_weights, dim=-1)\n",
    "inter_time_dist= LogNormalMixtureDistribution(\n",
    "    locs=locs,\n",
    "    log_scales=log_scales,\n",
    "    log_weights=log_weights,\n",
    "    mean_log_inter_time=model.mean_log_inter_time,\n",
    "    std_log_inter_time=model.std_log_inter_time\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "a = inter_time_dist.std_log_inter_time\n",
    "b = inter_time_dist.mean_log_inter_time\n",
    "loc = inter_time_dist.base_dist._component_distribution.loc\n",
    "variance = inter_time_dist.base_dist._component_distribution.variance\n",
    "log_weights = inter_time_dist.base_dist._mixture_distribution.logits\n",
    "(log_weights + a * loc + b + 0.5 * a ** 2 * variance).logsumexp(-1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:23:02.165852Z",
     "start_time": "2021-07-07T21:23:02.159894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.7428)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.std_log_inter_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\sum_k w_k \\exp(a * \\mu_k + b + a^2 * s_k^2 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:16:18.200766Z",
     "start_time": "2021-07-07T21:16:18.184785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.3827e-02,  2.4959e+00,  5.6551e+00,  9.6110e-01,  1.6284e+01,\n",
       "         1.0101e+00,  3.3274e+00,  4.1098e+01,  7.5747e+00,  1.8718e+00,\n",
       "        -1.3737e+00,  3.4822e+01,  8.1448e+00,  1.8703e+01, -2.8655e+00,\n",
       "         5.7551e+02,  6.7150e-01, -1.7918e+00,  4.2643e+00,  6.0229e+00,\n",
       "         1.8080e+01,  2.7982e+00,  7.7480e+00,  2.0706e+00,  9.0482e-01,\n",
       "         1.3496e+00,  1.0050e+01,  1.2869e+00,  3.7434e+00,  5.2432e+00,\n",
       "        -7.3683e-01,  7.0787e-01,  9.2646e+00,  2.1809e-01,  2.8906e+00,\n",
       "         1.1255e+00, -1.1422e+00, -5.0909e-01,  3.0159e+00,  5.1239e+00,\n",
       "         6.7002e-01,  1.7750e+02,  2.3232e+00,  2.4128e+00,  1.8722e+00,\n",
       "         5.1205e+00,  7.2887e+00,  2.4980e-01,  1.3400e+00, -3.3546e+00,\n",
       "         3.8095e-01,  1.0204e+01, -3.1551e+00,  2.8819e-01,  6.4451e+01,\n",
       "        -2.5328e+00,  1.1071e+01, -7.6883e-01,  3.5442e+01, -4.6588e-01,\n",
       "        -2.8729e+00,  1.5414e+00,  3.0847e+02, -2.2779e+00],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a * loc[-1][2]) + b+ (0.5*variance[-1][2] * a**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:06:57.172361Z",
     "start_time": "2021-07-07T21:06:57.152414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.7719e+00, -2.4933e+00,  3.2127e-01, -2.6305e+00,  1.0638e+01,\n",
       "        -3.2886e+00, -1.0176e+00,  3.4976e+01,  2.0127e+00, -2.0520e+00,\n",
       "        -5.5669e+00,  2.8941e+01,  2.8677e+00,  1.3045e+01, -6.8826e+00,\n",
       "         5.6830e+02, -3.5138e+00, -5.4087e+00, -9.0726e-01,  1.1996e+00,\n",
       "         1.2257e+01, -1.5404e+00,  2.2187e+00, -2.3010e+00, -2.3071e+00,\n",
       "        -1.6139e+00,  4.6739e+00, -1.7721e+00, -1.1501e+00,  3.2883e-01,\n",
       "        -4.4762e+00, -3.1934e+00,  3.7948e+00, -3.6728e+00, -1.2871e+00,\n",
       "        -3.8649e+00, -4.4348e+00, -4.7654e+00, -1.4894e+00, -5.6178e-01,\n",
       "        -3.3523e+00,  1.7069e+02, -2.0465e+00, -2.2022e+00, -2.6822e+00,\n",
       "         4.8712e-02,  1.9735e+00, -3.6402e+00, -2.7830e+00, -6.9390e+00,\n",
       "        -4.2677e+00,  4.9924e+00, -6.5031e+00, -3.9428e+00,  5.8077e+01,\n",
       "        -6.1407e+00,  5.1859e+00, -4.4552e+00,  2.9361e+01, -3.4964e+00,\n",
       "        -6.2605e+00, -2.7168e+00,  3.0133e+02, -5.2975e+00],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(log_weights + a * loc + b + 0.5 * a ** 2 * variance)[-1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T21:06:22.590410Z",
     "start_time": "2021-07-07T21:06:22.578481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(568.3008, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = inter_time_dist.std_log_inter_time\n",
    "b = inter_time_dist.mean_log_inter_time\n",
    "loc = inter_time_dist.base_dist._component_distribution.loc\n",
    "variance = inter_time_dist.base_dist._component_distribution.variance\n",
    "log_weights = inter_time_dist.base_dist._mixture_distribution.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
