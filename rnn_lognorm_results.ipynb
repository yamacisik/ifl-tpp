{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:31:24.753549Z",
     "start_time": "2021-07-02T20:31:24.745549Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data import Sequence,SequenceDataset\n",
    "from model import LogNormMix\n",
    "\n",
    "from copy import deepcopy\n",
    "from util import pad_sequence\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:20:39.879293Z",
     "start_time": "2021-07-02T06:20:39.832546Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = 'data/simulated/hawkes_synthetic_random_2d_20191130-180837.pkl'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "dataset = np.load(dataset_name,allow_pickle = True)\n",
    "\n",
    "## Modify the dataset for IFTPL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:20:40.624945Z",
     "start_time": "2021-07-02T06:20:40.609729Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "# for i  in range(len(dataset['timestamps'])):\n",
    "for i  in range(400):\n",
    "    sequence = {'t_start':0,'t_end' :200, 'arrival_times':dataset['timestamps'][i].tolist(),'marks' :dataset['types'][i].tolist()}\n",
    "    \n",
    "    sequences.append(sequence)\n",
    "    \n",
    "simulated_data = {'sequences':sequences,'num_marks':2}\n",
    "\n",
    "# np.save('data/simulated/sample_hawkes',simulated_data,allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:39:16.963211Z",
     "start_time": "2021-07-02T06:39:16.902822Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# Model config\n",
    "context_size = 64                 # Size of the RNN hidden vector\n",
    "mark_embedding_size = 32          # Size of the mark embedding (used as RNN input)\n",
    "num_mix_components = 64           # Number of components for a mixture model\n",
    "rnn_type = \"GRU\"                  # What RNN to use as an encoder {\"RNN\", \"GRU\", \"LSTM\"}\n",
    "\n",
    "# Training config\n",
    "batch_size = 32        # Number of sequences in a batch\n",
    "regularization = 1e-5  # L2 regularization parameter\n",
    "learning_rate = 1e-3   # Learning rate for Adam optimizer\n",
    "max_epochs = 5      # For how many epochs to train\n",
    "display_step = 1      # Display training statistics after every display_step\n",
    "patience = 50          # After how many consecutive epochs without improvement of val loss to stop training\n",
    "\n",
    "test_dataset = {}\n",
    "test_dataset['sequences']= [{'t_start': 0,'t_end': 10,'arrival_times': [1,2],'marks':[0,1]},\n",
    "                            {'t_start': 0,'t_end': 10,'arrival_times': [1,3,5],'marks':[0,1,0]},\n",
    "                            {'t_start': 0,'t_end': 10,'arrival_times': [1,7,8,9],'marks':[0,1,1,0]},\n",
    "                            {'t_start': 0,   't_end': 10,'arrival_times': [5,9],'marks':[1,1]}  ]\n",
    "\n",
    "def get_inter_times(seq: dict):\n",
    "    \"\"\"Get inter-event times from a sequence.\"\"\"\n",
    "    return np.ediff1d(np.concatenate([[seq[\"t_start\"]], seq[\"arrival_times\"], [seq[\"t_end\"]]]))\n",
    "\n",
    "sequences = [\n",
    "    Sequence(\n",
    "        inter_times=get_inter_times(seq),\n",
    "        marks=seq.get(\"marks\"),\n",
    "        t_start=seq.get(\"t_start\"),\n",
    "        t_end=seq.get(\"t_end\")\n",
    "    )\n",
    "    for seq in simulated_data[\"sequences\"]\n",
    "]\n",
    "\n",
    "seed = 0\n",
    "batch_size = 5\n",
    "dataset = SequenceDataset(sequences=sequences, num_marks=2)\n",
    "\n",
    "d_train, d_val, d_test = dataset.train_val_test_split(seed=None,shuffle = False)\n",
    "\n",
    "training_events= d_train.total_num_events\n",
    "\n",
    "dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# dataset_name = 'data/stack_overflow.pkl'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "# dataset = torch.load(dataset_name)\n",
    "\n",
    "\n",
    "# def get_inter_times(seq: dict):\n",
    "#     \"\"\"Get inter-event times from a sequence.\"\"\"\n",
    "#     return np.ediff1d(np.concatenate([[seq[\"t_start\"]], seq[\"arrival_times\"], [seq[\"t_end\"]]]))\n",
    "\n",
    "\n",
    "# sequences = [\n",
    "#     Sequence(\n",
    "#         inter_times=get_inter_times(seq),\n",
    "#         marks=seq.get(\"marks\"),\n",
    "#         t_start=seq.get(\"t_start\"),\n",
    "#         t_end=seq.get(\"t_end\")\n",
    "#     )\n",
    "#     for seq in dataset[\"sequences\"]\n",
    "# ]\n",
    "# dataset = SequenceDataset(sequences=sequences, num_marks=dataset.get(\"num_marks\", 1))\n",
    "\n",
    "# d_train, d_val, d_test = dataset.train_val_test_split(seed=seed)\n",
    "\n",
    "# dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=True)\n",
    "# dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "# dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:43:12.443045Z",
     "start_time": "2021-07-02T06:43:12.421432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(inter_times=[5, 176], marks=[5, 176], mask=[5, 176])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = []\n",
    "for batch in dl_test:\n",
    "    batches.append(batch)\n",
    "batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:42:20.979500Z",
     "start_time": "2021-07-02T06:42:20.966464Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl_test.dataset.sequences[0].inter_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:39:23.795033Z",
     "start_time": "2021-07-02T06:39:23.766640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the model\n",
    "print('Building model...')\n",
    "mean_log_inter_time, std_log_inter_time = d_train.get_inter_time_statistics()\n",
    "\n",
    "model =LogNormMix(\n",
    "    num_marks=d_train.num_marks,\n",
    "    mean_log_inter_time=mean_log_inter_time,\n",
    "    std_log_inter_time=std_log_inter_time,\n",
    "    context_size=context_size,\n",
    "    mark_embedding_size=mark_embedding_size,\n",
    "    rnn_type=rnn_type,\n",
    "    num_mix_components=num_mix_components,\n",
    ")\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), weight_decay=regularization, lr=learning_rate)\n",
    "# Traning\n",
    "print('Starting training...')\n",
    "\n",
    "def aggregate_loss_over_dataloader(dl):\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            total_loss += -model.log_prob(batch).sum()\n",
    "            total_count += batch.mask.sum().item()\n",
    "    return total_loss / total_count\n",
    "\n",
    "\n",
    "impatient = 0\n",
    "best_loss = np.inf\n",
    "best_model = deepcopy(model.state_dict())\n",
    "training_val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:40:45.901675Z",
     "start_time": "2021-07-02T06:39:28.919304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: Training loss = 1.8333, loss_val = 1.8100\n",
      "Epoch    1: Training loss = 1.7969, loss_val = 1.8056\n",
      "Epoch    2: Training loss = 1.7926, loss_val = 1.8009\n",
      "Epoch    3: Training loss = 1.7895, loss_val = 1.7984\n",
      "Epoch    4: Training loss = 1.7871, loss_val = 1.7972\n",
      "Negative log-likelihood:\n",
      " - Train: 1.8\n",
      " - Val:   1.8\n",
      " - Test:  1.7\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    model.train()\n",
    "    for batch in dl_train:\n",
    "        opt.zero_grad()\n",
    "        # loss = -model.log_prob(batch)\n",
    "        loss = -model.log_prob(batch).sum()\n",
    "        loss.backward()\n",
    "        epoch_train_loss += loss.detach()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_val = aggregate_loss_over_dataloader(dl_val)\n",
    "        loss_test = aggregate_loss_over_dataloader(dl_test)\n",
    "\n",
    "        training_val_losses.append(loss_val)\n",
    "\n",
    "    if (best_loss - loss_val) < 1e-4:\n",
    "        impatient += 1\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    else:\n",
    "        best_loss = loss_val\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "        impatient = 0\n",
    "\n",
    "    if impatient >= patience:\n",
    "        print(f'Breaking due to early stopping at epoch {epoch}')\n",
    "        break\n",
    "    \n",
    "    \n",
    "    epoch_train_loss = epoch_train_loss/training_events\n",
    "\n",
    "    if epoch % display_step == 0:\n",
    "        print(f\"Epoch {epoch:4d}: Training loss = {epoch_train_loss.item():.4f}, loss_val = {loss_val:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# All training & testing sequences stacked into a single batch\n",
    "with torch.no_grad():\n",
    "    final_loss_train = aggregate_loss_over_dataloader(dl_train)\n",
    "    final_loss_val = aggregate_loss_over_dataloader(dl_val)\n",
    "    final_loss_test = aggregate_loss_over_dataloader(dl_test)\n",
    "\n",
    "print(f'Negative log-likelihood:\\n'\n",
    "      f' - Train: {final_loss_train:.4f}\\n'\n",
    "      f' - Val:   {final_loss_val:.4f}\\n'\n",
    "      f' - Test:  {final_loss_test:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:42.612274Z",
     "start_time": "2021-07-02T06:47:42.587254Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction_for_all_events(model,dl):\n",
    "    \n",
    "    total_num_events = dl.dataset.total_num_events\n",
    "    all_event_time_predictions = []\n",
    "    all_event_time_values = []\n",
    "    all_actual_marks = []\n",
    "    all_predicted_marks = []\n",
    "    \n",
    "    \n",
    "    for batch in dl:\n",
    "        \n",
    "        lengths = batch.mask.sum(-1)-1 ## Minus 1 because they also calculate the survival for the end of time.\n",
    "\n",
    "        \n",
    "        features = model.get_features(batch)\n",
    "        context = model.get_context(features)\n",
    "        inter_time_dist = model.get_inter_time_dist(context)\n",
    "        inter_times = batch.inter_times.clamp(1e-10)\n",
    "        \n",
    "        ## Arrival Time Prediction\n",
    "        predicted_times = inter_time_dist.mean        \n",
    "        all_predicted_times = torch.nn.utils.rnn.pack_padded_sequence(predicted_times.T,lengths,batch_first=False,enforce_sorted=False)[0]\n",
    "        all_actual_times = torch.nn.utils.rnn.pack_padded_sequence(inter_times.T,lengths,batch_first=False,enforce_sorted=False)[0]\n",
    "        all_event_time_values.append(all_actual_times[:-1])\n",
    "        all_event_time_predictions.append(all_predicted_times[:-1])\n",
    "        \n",
    "\n",
    "\n",
    "#         ## Mark Prediction\n",
    "        predicted_marks= torch.log_softmax(model.mark_linear(context), dim=-1).argmax(-1)\n",
    "        predicted_marks = torch.nn.utils.rnn.pack_padded_sequence(predicted_marks.T,lengths,batch_first=False,enforce_sorted=False)[0]\n",
    "        actual_marks = torch.nn.utils.rnn.pack_padded_sequence(batch.marks.T,lengths,batch_first=False,enforce_sorted =False)[0]\n",
    "        all_actual_marks.append(actual_marks)\n",
    "        all_predicted_marks.append(predicted_marks)\n",
    "        \n",
    "#         all_event_accuracy = (predicted_marks ==batch.marks)*batch.mask\n",
    "#         all_event_accuracies.append(all_event_accuracy)\n",
    "#         last_event_accuracy = all_event_accuracy[x_index,y_index]\n",
    "#         last_event_accuracies.append(last_event_accuracy)\n",
    "        \n",
    "    \n",
    "#     last_event_rmse = (torch.cat(last_event_errors,axis = 0)**2).mean().sqrt()\n",
    "#     all_event_rmse = ((torch.cat(total_errors,-1)**2).sum()/total_num_events).sqrt()\n",
    "#     all_event_accuracy = torch.cat(all_event_accuracies,-1).sum()/total_num_events\n",
    "#     last_event_accuracy = torch.cat(last_event_accuracies,0)\n",
    "# #     last_event_accuracy = last_event_accuracy.sum()/len(last_event_accuracy)\n",
    "    \n",
    "#     return last_event_rmse,all_event_rmse,all_event_accuracy,last_event_accuracy\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_event_time_values = torch.cat(all_event_time_values)\n",
    "    all_event_time_predictions = torch.cat(all_event_time_predictions)\n",
    "    all_actual_marks = torch.cat(all_actual_marks)\n",
    "    all_predicted_marks = torch.cat(all_predicted_marks)\n",
    "    \n",
    "    return all_event_time_values,all_event_time_predictions,all_actual_marks,all_predicted_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:55.177158Z",
     "start_time": "2021-07-02T06:47:55.157159Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction_for_last_events(model,dl):\n",
    "    \n",
    "    event_time_predictions = []\n",
    "    event_time_values = []\n",
    "    actual_marks = []\n",
    "    predicted_marks = []\n",
    "    \n",
    "    \n",
    "    for batch in dl:\n",
    "        \n",
    "        y_index = batch.mask.sum(-1).long() -1  ## Minus 1 because they also calculate the survival for the end of time.\n",
    "        features = model.get_features(batch)\n",
    "        context = model.get_context(features)\n",
    "        inter_time_dist = model.get_inter_time_dist(context)\n",
    "        inter_times = batch.inter_times.clamp(1e-10)\n",
    "        x_index = torch.arange(0,len(inter_times))\n",
    "        \n",
    "        ## Arrival Time Prediction\n",
    "        actual_time = inter_times[x_index,y_index]   \n",
    "        predicted_time = inter_time_dist.mean[x_index,y_index]     \n",
    "\n",
    "        ## Mark Prediction\n",
    "        actual_mark = batch.marks[x_index,y_index]\n",
    "        predicted_mark= torch.log_softmax(model.mark_linear(context), dim=-1).argmax(-1)[x_index,y_index]\n",
    "        \n",
    "        event_time_predictions.append(predicted_time)\n",
    "        event_time_values.append(actual_time)\n",
    "        actual_marks.append(actual_mark)\n",
    "        predicted_marks.append(predicted_mark)\n",
    "    \n",
    "    event_time_values = torch.cat(event_time_values)\n",
    "    event_time_predictions = torch.cat(event_time_predictions)\n",
    "    actual_marks = torch.cat(actual_marks)\n",
    "    predicted_marks = torch.cat(predicted_marks)\n",
    "    \n",
    "    return event_time_values,event_time_predictions,actual_marks,predicted_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:36:44.653532Z",
     "start_time": "2021-07-02T07:36:44.635534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.marks.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:56.679048Z",
     "start_time": "2021-07-02T06:47:56.294883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218.84422302246094\n",
      "0.550259965337955\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_all_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy())\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:47:57.979251Z",
     "start_time": "2021-07-02T06:47:57.597646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.017912864685059\n",
      "0.4571428571428572\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_last_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy())\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:52:42.635352Z",
     "start_time": "2021-07-02T07:52:42.617299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.marks.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:33:17.097874Z",
     "start_time": "2021-07-02T20:33:16.378156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_name = 'simulated'\n",
    "num_marks = 2\n",
    "dataset_path = 'data/' + dataset_name + '/'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "\n",
    "with open(dataset_path + 'train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(dataset_path + 'valid.pkl', 'rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(dataset_path + 'test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "\n",
    "def create_seq_data_set(dataset, num_marks,device):\n",
    "    sequences = [\n",
    "        Sequence(\n",
    "            inter_times=get_inter_times(seq),\n",
    "            marks=seq.get(\"marks\"),\n",
    "            t_start=seq.get(\"t_start\"),\n",
    "            t_end=seq.get(\"t_end\"),device = device\n",
    "        )\n",
    "        for seq in dataset[\"sequences\"]\n",
    "    ]\n",
    "    dataset = SequenceDataset(sequences=sequences, num_marks=num_marks)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "device ='cpu'\n",
    "d_train = create_seq_data_set(train, num_marks,device)\n",
    "d_val = create_seq_data_set(valid, num_marks,device)\n",
    "d_test = create_seq_data_set(test, num_marks,device)\n",
    "\n",
    "dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "print('Building model...')\n",
    "mean_log_inter_time, std_log_inter_time = d_train.get_inter_time_statistics()\n",
    "\n",
    "model =LogNormMix(\n",
    "    num_marks=d_train.num_marks,\n",
    "    mean_log_inter_time=mean_log_inter_time,\n",
    "    std_log_inter_time=std_log_inter_time,\n",
    "    context_size=context_size,\n",
    "    mark_embedding_size=mark_embedding_size,\n",
    "    rnn_type=rnn_type,\n",
    "    num_mix_components=num_mix_components,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model_dict =torch.load('intensity_free_modelsimulated',map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:33:52.771400Z",
     "start_time": "2021-07-02T20:33:49.743532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.669763565063477\n",
      "0.588495575221239\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_last_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy())\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mimic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:35:02.539203Z",
     "start_time": "2021-07-02T20:35:02.475181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_name = 'mimic'\n",
    "num_marks = 75\n",
    "dataset_path = 'data/' + dataset_name + '/'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "\n",
    "with open(dataset_path + 'train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(dataset_path + 'valid.pkl', 'rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(dataset_path + 'test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "\n",
    "def create_seq_data_set(dataset, num_marks,device):\n",
    "    sequences = [\n",
    "        Sequence(\n",
    "            inter_times=get_inter_times(seq),\n",
    "            marks=seq.get(\"marks\"),\n",
    "            t_start=seq.get(\"t_start\"),\n",
    "            t_end=seq.get(\"t_end\"),device = device\n",
    "        )\n",
    "        for seq in dataset[\"sequences\"]\n",
    "    ]\n",
    "    dataset = SequenceDataset(sequences=sequences, num_marks=num_marks)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "device ='cpu'\n",
    "d_train = create_seq_data_set(train, num_marks,device)\n",
    "d_val = create_seq_data_set(valid, num_marks,device)\n",
    "d_test = create_seq_data_set(test, num_marks,device)\n",
    "\n",
    "dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "print('Building model...')\n",
    "mean_log_inter_time, std_log_inter_time = d_train.get_inter_time_statistics()\n",
    "\n",
    "model =LogNormMix(\n",
    "    num_marks=d_train.num_marks,\n",
    "    mean_log_inter_time=mean_log_inter_time,\n",
    "    std_log_inter_time=std_log_inter_time,\n",
    "    context_size=context_size,\n",
    "    mark_embedding_size=mark_embedding_size,\n",
    "    rnn_type=rnn_type,\n",
    "    num_mix_components=num_mix_components,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model_dict =torch.load('intensity_free_modelmimic',map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:36:30.496829Z",
     "start_time": "2021-07-02T20:36:30.432802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "0.8769230769230769\n"
     ]
    }
   ],
   "source": [
    "actual_times,predicted_times,actual_marks,predicted_marks = get_prediction_for_last_events(model,dl_test)\n",
    "\n",
    "RMSE = (((predicted_times - actual_times)/actual_times)**2).mean().sqrt()\n",
    "f1= f1_score(predicted_marks.detach().numpy(),actual_marks.detach().numpy(),average = 'micro')\n",
    "\n",
    "print(RMSE.item())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:41:26.736997Z",
     "start_time": "2021-07-02T20:41:26.728990Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch in dl_test:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:41:31.843311Z",
     "start_time": "2021-07-02T20:41:31.827311Z"
    }
   },
   "outputs": [],
   "source": [
    "y_index = batch.mask.sum(\n",
    "    -1).long() - 1  ## Minus 1 because they also calculate the survival for the end of time.\n",
    "features = model.get_features(batch)\n",
    "context = model.get_context(features)\n",
    "inter_time_dist = model.get_inter_time_dist(context)\n",
    "inter_times = batch.inter_times.clamp(1e-10)\n",
    "x_index = torch.arange(0, len(inter_times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:42:38.138888Z",
     "start_time": "2021-07-02T20:42:38.122885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-10, 5.7692e-02, 2.8846e-01, 5.6538e+00, 0.0000e+00],\n",
       "        [1.0000e-10, 1.9231e-02, 2.1154e-01, 5.7692e+00, 0.0000e+00],\n",
       "        [1.0000e-10, 4.6154e-01, 2.3846e+00, 3.1538e+00, 0.0000e+00],\n",
       "        [1.0000e-10, 2.1923e+00, 7.3077e-01, 1.4615e+00, 1.6154e+00],\n",
       "        [1.0000e-10, 3.0769e-01, 3.4615e-01, 5.3462e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.inter_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:38:56.426674Z",
     "start_time": "2021-07-02T20:38:56.402714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5769, 0.0385, 0.9038, 0.0769, 0.0385, 1.3846, 0.0962, 0.4231, 0.3077,\n",
       "        0.8077, 0.5769, 1.4038, 0.2692, 0.0769, 0.0577, 1.1923, 2.3077, 0.0577,\n",
       "        2.4423, 0.1154, 0.1346, 0.2692, 0.3269, 0.1731, 0.1154, 0.3846, 0.0962,\n",
       "        0.0769, 0.0962, 0.1538, 1.9423, 1.1731, 0.1154, 0.7308, 0.2500, 0.2500,\n",
       "        0.4423, 0.4615, 1.7500, 1.6538, 0.1923, 0.9423, 1.3269, 0.0385, 0.0577,\n",
       "        0.4038, 0.7885, 0.2115, 0.1538, 0.1154, 1.3846, 0.0192, 0.0385, 0.1346,\n",
       "        0.2500, 0.1731, 1.0385, 0.5385, 2.6154, 0.0769, 0.2885, 0.2115, 2.3846,\n",
       "        1.4615, 0.3462])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:44:29.895176Z",
     "start_time": "2021-07-02T20:44:29.880214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  1,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0],\n",
       "        [ 8,  8,  8,  0,  0],\n",
       "        [13,  1,  1,  1,  0],\n",
       "        [20, 20, 20,  0,  0]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:45:41.181164Z",
     "start_time": "2021-07-02T20:45:41.157431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t_start': 0,\n",
       " 't_end': 6,\n",
       " 'arrival_times': [0.0,\n",
       "  2.192307710647583,\n",
       "  2.923076868057251,\n",
       "  4.384615421295166],\n",
       " 'marks': [13, 1, 1, 1]}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['sequences'][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:45:59.541928Z",
     "start_time": "2021-07-02T20:45:59.526288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.615384578704834"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6  - 4.384615421295166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:46:43.666963Z",
     "start_time": "2021-07-02T20:46:43.642910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-10, 5.7692e-02, 2.8846e-01, 5.6538e+00, 1.0000e-10],\n",
       "        [1.0000e-10, 1.9231e-02, 2.1154e-01, 5.7692e+00, 1.0000e-10],\n",
       "        [1.0000e-10, 4.6154e-01, 2.3846e+00, 3.1538e+00, 1.0000e-10],\n",
       "        [1.0000e-10, 2.1923e+00, 7.3077e-01, 1.4615e+00, 1.6154e+00],\n",
       "        [1.0000e-10, 3.0769e-01, 3.4615e-01, 5.3462e+00, 1.0000e-10]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T20:53:14.948784Z",
     "start_time": "2021-07-02T20:53:14.932797Z"
    }
   },
   "outputs": [],
   "source": [
    "a = inter_time_dist.std_log_inter_time\n",
    "b = inter_time_dist.mean_log_inter_time\n",
    "loc = inter_time_dist.base_dist._component_distribution.loc\n",
    "variance = inter_time_dist.base_dist._component_distribution.variance\n",
    "log_weights = inter_time_dist.base_dist._mixture_distribution.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
